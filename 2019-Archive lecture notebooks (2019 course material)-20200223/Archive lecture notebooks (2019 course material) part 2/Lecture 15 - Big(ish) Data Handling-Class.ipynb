{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big(ish) Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage, Data Formats, Out-of-core processing and Data Accessing Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Big(ish) Data Handling](#Big%28ish%29-Data-Handling)\n",
    "\t* [Data Storage, Data Formats, Out-of-core processing and Data Accessing Strategies](#Data-Storage,-Data-Formats,-Out-of-core-processing-and-Data-Accessing-Strategies)\n",
    "\t\t* [Learning Outcomes](#Learning-Outcomes)\n",
    "\t\t\t* [Massey Lab Instructions](#Massey-Lab-Instructions)\n",
    "* [Splitting large files into smaller chunks - the naive approach](#Splitting-large-files-into-smaller-chunks---the-naive-approach)\n",
    "* [Optimising Pandas DataFrame Accesses](#Optimising-Pandas-DataFrame-Accesses)\n",
    "* [Strategies for Processing Large Datasets](#Strategies-for-Processing-Large-Datasets)\n",
    "* [My dataset is bigger than my RAM](#My-dataset-is-bigger-than-my-RAM)\n",
    "\t* [HDF5](#HDF5)\n",
    "\t\t* &nbsp;\n",
    "\t\t\t* [In the above:](#In-the-above:)\n",
    "\t\t* [Read subsets of data from disk using HDF5](#Read-subsets-of-data-from-disk-using-HDF5)\n",
    "* [Out-of-core processing with the Dask DataFrame](#Out-of-core-processing-with-the-Dask-DataFrame)\n",
    "\t* &nbsp;\n",
    "\t\t* [From now on we will pretend that our dataset is > RAM or large enough to take up too much of our RAM. Working with an actual dataset that is > RAM, will slow down all the examples in class considerably, but this is left as an exercise for you to perform out of class.](#From-now-on-we-will-pretend-that-our-dataset-is->-RAM-or-large-enough-to-take-up-too-much-of-our-RAM.-Working-with-an-actual-dataset-that-is->-RAM,-will-slow-down-all-the-examples-in-class-considerably,-but-this-is-left-as-an-exercise-for-you-to-perform-out-of-class.)\n",
    "\t\t* [Compare CSV to HDF5 speeds](#Compare-CSV-to-HDF5-speeds)\n",
    "* [Sampling](#Sampling)\n",
    "\t* &nbsp;\n",
    "\t\t* [Density Map of NYC Rides](#Density-Map-of-NYC-Rides)\n",
    "* [Store text/string data more efficiently with categoricals](#Store-text/string-data-more-efficiently-with-categoricals)\n",
    "\t* &nbsp;\n",
    "\t\t* [Let's compare again our categorised HDF5, with the previous execution times:](#Let's-compare-again-our-categorised-HDF5,-with-the-previous-execution-times:)\n",
    "* [More Efficient On-Disk Storage Combined with Indexes](#More-Efficient-On-Disk-Storage-Combined-with-Indexes)\n",
    "* [SQL Lite](#SQL-Lite)\n",
    "\t* [Summary of Strategies For Dealing with Inconvenient Datasets](#Summary-of-Strategies-For-Dealing-with-Inconvenient-Datasets)\n",
    "* [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this lecture, you should be able to:\n",
    "\n",
    "* work with 'inconveniently large' datasets\n",
    "* chunk large datasets \n",
    "* convert datasets to HDF5 \n",
    "* perform out-of-core processing\n",
    "* optimise dataset storage\n",
    "* interface with sqlite datasets\n",
    "\n",
    "\n",
    "\n",
    "This notebook will use the NYC Taxi Trip 2013-2014 dataset made up of 12 files, one for each month. The complete dataset is about 31GB. In the interest of time during class, the examples here will only use a portion of the data from the month of January. You are encouraged to employ the strategies below on the entire dataset in your own time. \n",
    "\n",
    "Dataset and selected examples drawn from http://www.andresmh.com/nyctaxitrips/. More info on the NYC Taxi Trip 2013-2014 dataset can be found here http://chriswhong.com/open-data/foil_nyc_taxi/ and http://hafen.github.io/taxi/#background. Selected material on Dask usage, sourced from https://github.com/dask/dask-tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Massey Lab Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. download trip_data_1a.csv and have it ready for class (especially if using a laptop)\n",
    "2. save your file in C:/I folder because your H drive is likely to run out of space\n",
    "3. write all files to the C drive for the same reason as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipython_memwatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipython_memwatcher import MemWatcher\n",
    "mw = MemWatcher()\n",
    "mw.start_watching_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of Pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Wes McKinney, the author of Python Pandas, there are some serious limitations inherent in Pandas when it comes to dealing with larger datasets that were not considered when the project to develop Pandas was initiated:\n",
    "\n",
    "> To put it simply, we weren't thinking about analyzing 100 GB or 1 TB datasets in 2011. Nowadays, my rule of thumb for pandas is that you should have 5 to 10 times as much RAM as the size of your dataset. So if you have a 10 GB dataset, you should really have about 64, preferably 128 GB of RAM if you want to avoid memory management problems. This comes as a shock to users who expect to be able to analyze datasets that are within a factor of 2 or 3 the size of their computer's RAM.\n",
    "\n",
    ">> **pandas rule of thumb: have 5 to 10 times as much RAM as the size of your dataset**\n",
    "\n",
    "> There are additional, hidden memory killers in the project, like the way that we use Python objects (like strings) for many internal details, so **it's not unusual to see a dataset that is 5GB on disk take up 20GB or more in memory**. It's an overall bad situation for large datasets.\n",
    "\n",
    "Source: http://wesmckinney.com/blog/apache-arrow-pandas-internals/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting large files into smaller chunks - the naive approach\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'D:/158739_2019/2019_Python_notebooks/Block_2/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that the file we are dealing with in this example cannot fit into out working memory (RAM). What can we do?....\n",
    "\n",
    "Well, given a very large csv file, often the most simplistic approach to working with it is to split it into multiple smaller csv file chunks, and then process each one in turn.\n",
    "\n",
    "Here we will re-write each chunk to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "z = zipfile.ZipFile(file_path + 'trip_data_1a.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the original trip_data_1.csv data file has over 14M records - \n",
    "# the trip_data_1a.csv has 5M rows which we will divide here into three chunks\n",
    "chunksize = 2000000\n",
    "i = 0\n",
    "f = ['a','b','c']\n",
    "for df in pd.read_csv(z.open('trip_data_1a.csv'), chunksize=chunksize):\n",
    "        df.to_csv(file_path + 'trip_data_1a_' + f[i] + '.csv', index=None)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the original trip_data_1.csv data file has over 14M records - \n",
    "# the trip_data_1a.csv has 5M rows which we will divide here into three chunks\n",
    "chunksize = 2000000\n",
    "i = 0\n",
    "f = ['a','b','c']\n",
    "for df in pd.read_csv(file_path + 'trip_data_1a.csv', chunksize=chunksize):\n",
    "        df.to_csv(file_path + 'trip_data_1a_' + f[i] + '.csv', index=None)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising Pandas DataFrame Accesses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing heavily intensive computations on dataframes of a large size, it is imperative that some form of profiling and optimisation is carried out.\n",
    "\n",
    "The way we process and access pandas dataframes can have significant effects on the runtime performance that differs in orders of magnitude between various options.  \n",
    "\n",
    "Since we can fit all our data into memory, let's do this in order to explore some pandas performance characteristics.\n",
    "\n",
    "http://jose-coto.com/slicing-methods-pandas\n",
    "https://stackoverflow.com/questions/28757389/loc-vs-iloc-vs-ix-vs-at-vs-iat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read it as a zipped file or a csv\n",
    "df = pd.read_csv(z.open('trip_data_1a.csv'), infer_datetime_format=True, parse_dates=['pickup_datetime', 'dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path + 'trip_data_1a.csv', infer_datetime_format=True, parse_dates=['pickup_datetime', 'dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df = pd.read_hdf('../datasets/trip_data_1a.h5', '/data', start=0, stop=1000000)\n",
    "#df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider looping through a dataframe that has 1M rows.\n",
    "\n",
    "iat - position based works similarly to iloc.\n",
    "\n",
    "Advantage over iloc is that this is faster.\n",
    "\n",
    "Disadvantage is that you can't use arrays for indexers. Cannot assign new indices and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dataframe_access_using_iat(df):\n",
    "    for i in range(len(df)):\n",
    "        df.iat[i, 0]\n",
    "        \n",
    "%time dataframe_access_using_iat(df[:1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "df.iat[10000000, 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "df.iat[ [0,1] , 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "df.iat[0, 'medallion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at - label based works very similarly to loc for scalar indexers.\n",
    "\n",
    "Advantage over loc is that this is faster. \n",
    "\n",
    "Disadvantage is that you can't use arrays for indexers. Cannot assign new indices and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_access_using_at(df):\n",
    "    for i in range(len(df)):\n",
    "        df.at[i, 'medallion']\n",
    "\n",
    "%time dataframe_access_using_at(df[:1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "df.at[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "df.at[10000000, 'medallion'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iloc - position based\n",
    "Similar to loc except with positions rather that index values. However, you cannot assign new columns or indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_access_using_iloc(df):\n",
    "    for i in range(len(df)):\n",
    "        df.iloc[i, 0]\n",
    "\n",
    "%time dataframe_access_using_iloc(df[:1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "df.iloc[ [0,1] , 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "df.iloc[ [0,1], 'medallion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ix - It accepts label based or index positional arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** *.ix* operator is now deprecated but the *.loc* access operator still remains as untested. Using the approach above, create a test for this operator below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loc - label based\n",
    "Allows you to pass 1-D arrays as indexers. Arrays can be either slices (subsets) of the index or column, or they can be boolean arrays which are equal in length to the index or columns.\n",
    "\n",
    "Special Note: when a scalar indexer is passed, loc can assign a new index or column value that didn't exist before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies for Processing Large Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most python modules and machine learning algorithms assume that all the data can be loaded into memory.\n",
    "\n",
    "This is increasingly becoming unrealistic with the amount of data that is being generated, stored and used for data analysis.\n",
    "\n",
    "This creates considerable challenges. When the data is in the order of terabytes, then Hadoop distributed processing on clusters of commodity machines is the standard approach.\n",
    "\n",
    "Such infrastructure is not always available. \n",
    "\n",
    "When the datasets are in tens of gigabytes in size, Hadoop is an overkill and is often not the preferred option. \n",
    "\n",
    "Data that is larger than the size of a machine's RAM capacity is not Big Data, but it is large and inconvenient to work with. The challenges with this kind of data are:\n",
    "\n",
    "1. execution runtime\n",
    "2. working with libraries that enable processing the data without bringing all of it into RAM (out-of-core processing)\n",
    "3. ensuring that dataset file formats are optimised for out-of-core processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, since all of our data fits into the working memory, we can perform pretty fast computations on millions of records since the CPU is not having to access the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.passenger_count.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.trip_time_in_secs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My dataset is bigger than my RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this happens and you only have access to the tools covered so far, then you can resort to chunking/splitting the dataset as shown above and working on each part in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is extremely tedious. Do not do this if you can avoid it. Usually you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are far better strategies available to us. \n",
    "\n",
    "**Out-of-core processing** allows us to get around this problem is. However, it can still be very slow when the dataset is stored on disk in a non-binary format like csv. \n",
    "\n",
    "Files that are not saved in a **binary compressed format**, are not stored optimally and must also be interpreted as they are read from disk, which slows down the read time. \n",
    "\n",
    "Therefore, saving large files in a binary format instead of a text format like a csv, will ensure that the data is stored and read in future in a much more efficient manner.\n",
    "\n",
    "Saving datasets in specific binary formats also allows us to **filter and read into memory only subsets of data** that meet a required condition, which can significantly limit the RAM requirement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HDF5** (http://www.h5py.org/) is a binary format for efficiently storing and accessing data which makes it much more suitable for sharing and out-of-core processing.\n",
    "\n",
    "Therefore, when working with very large files, convert and store them in a binary format like HDF5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 stands for Hierarchical Data Format. It is a multipurpose hierarchical container format capable of storing large **numerical datasets** with their metadata. The specification is open and the tools are open source. \n",
    "\n",
    "It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HDF5 is portable and is extensible.\n",
    "\n",
    "An HDF5 file contains a hierarchy of numerical arrays (or datasets) organized within groups.\n",
    "\n",
    "A dataset can be stored in two ways: contiguously or chunked. The former stores a dataset in a contiguous buffer in the file, while the latter splits it uniformly in rectangular chunks organized in a B-tree.\n",
    "\n",
    "HDF5 also supports lossless compression of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path + 'trip_data_1a.csv', infer_datetime_format=True, parse_dates=['pickup_datetime', 'dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.to_hdf(file_path + 'trip_data_1a.h5', '/data', format='table', mode='w', data_columns=['trip_distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the above: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*format='table'* specifies that the file is searchable\n",
    "\n",
    "*mode='w' specifies write/create/overwrite \n",
    " \n",
    "*data_columns=['trip_distance']* cretes an index on the specified column so that we can select/search data on\n",
    "\n",
    "*/data is the object under which a given dataset is stored and is defined as a file path, you can define this name to whatever is suitable for the domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = pd.HDFStore(file_path + 'trip_data_1a.h5')\n",
    "hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['/data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets can be appended (using append) to the existing dataset in the HDF5 format, or new objects can be created within the same file under a different group path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.put('tables/t1', pd.DataFrame(np.random.rand(20,5)))\n",
    "hdf.put('tables/t2', pd.DataFrame(np.random.rand(10,3)))\n",
    "hdf.put('data/t1', pd.DataFrame(np.random.rand(15,2)))\n",
    "hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#release memory\n",
    "#del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read subsets of data from disk using HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a really large file that cannot fit into RAM, but has been converted into an HDF5 format, we can read into memory only portions of it that are of interest to us an to our immediate processing needs. \n",
    "\n",
    "We can bring into RAM only selected columns, and/or rows that satisfy a certain criterion. We can do this is if the HDF5 file is searchable, and we can specify filter constraints during file read I/O on the indexed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_hdf(file_path + 'trip_data_1a.h5', '/data', columns=['medallion','pickup_datetime', 'trip_distance'], where='trip_distance>10.0')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we imported a subset of columns above, based on a condition specified by the *where* argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a query that reads in data from the above file where the trip distance is greater than 5 miles and smaller than 15 miles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a query that reads in data from the above file where the number of passengers is greater than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice an error.\n",
    "\n",
    "This is because the *passenger_count* filed is not indexed and is not searchable. \n",
    "\n",
    "**Exercise:** Re-write the trip_data_1a file into HDF5, enabling the search for *passenger_count*, then perform the above query again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Read data from the above file, where the index number is between 10000 and 20000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-core processing with the Dask DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### From now on we will pretend that our dataset is > RAM or large enough to take up too much of our RAM. Working with an actual dataset that is > RAM, will slow down all the examples in class considerably, but this is left as an exercise for you to perform out of class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Dask`](https://dask.readthedocs.org/en/latest/) is a parallel computing library, designed to enable out-of-core processing. This means that most of the dataset can continue sitting on the hard disk while you are processing it, and it does not need to be read in it's entirety.\n",
    "\n",
    "Dask has a DataFrame class which functions just like the pandas counterpart; however, the **Dask DataFrame does not have all the functionality of a pandas DataFrame**.\n",
    "\n",
    "Dask can read large datasets from multiple file types.\n",
    "\n",
    "We will read the same data from a csv and HDF5 files and compare the out-of-core performance efficiencies of the two file types.\n",
    "\n",
    "We load this data into a dask dataframe using the `read_csv` function.  This has the exact same signature as `pandas.read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time ddf_csv = dd.read_csv(file_path + 'trip_data_1a.csv', \\\n",
    "                 parse_dates=['pickup_datetime', 'dropoff_datetime'])\n",
    "ddf_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ddf_csv.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ddf_hdf = dd.read_hdf(file_path + 'trip_data_1a.h5', '/data')\n",
    "ddf_hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_hdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(ddf_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(ddf_hdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling *.compute()* on a Dask DataFrame, converts the returned data into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare CSV to HDF5 speeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a simple computation that requires reading a column of our dataset and compare performance between CSV files and our newly created HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ddf_csv.passenger_count.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ddf_hdf.passenger_count.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ddf_csv[['vendor_id','passenger_count']].groupby('vendor_id').count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ddf_hdf[['vendor_id','passenger_count']].groupby('vendor_id').count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_hdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Construct a groupby query that groups the data by driver (medallion) and sums up all the trips travelled, listing the top 10. Do this using csv and hdf files and compare the execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ddf_hdf[['medallion','trip_distance']].groupby(ddf_hdf.medallion).count().compute().sort_values(by='trip_distance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes when you have an enormous amount of data, you do not need to visualise or compute all of it. As long as you have out-of-core processing capabilities, it is often enough just to sample the data instead of working with all of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's do some more processing of our data, but this time using the sampling strategy to handle the data size challenge,  and let's observe performance differences between accessing CSV and HDF5 files.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Map of NYC Rides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Bokeh to plot out a density map of all start and end locations for all taxi rides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sample of pickup and dropoff locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll pick out a few columns and use the `sample` method to pick out a random sample of 1% of all rides.  We'll then call `compute` on this data to convert the results into in-memory Pandas DataFrames that we can hand off to Bokeh for visualisation.\n",
    "\n",
    "We'll track our progress with the `ProgressBar` diagnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ddf_csv.sample(frac=0.01)\n",
    "print(len(sample))\n",
    "pickup = sample[['pickup_latitude', 'pickup_longitude']]\n",
    "with ProgressBar():\n",
    "    result = pickup.compute()  # compute to smaller Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(title=\"Pickup Locations\")\n",
    "p.scatter(result.pickup_longitude, result.pickup_latitude, size=3, alpha=0.2)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ddf_hdf.sample(frac=0.01)\n",
    "pickup = sample[['pickup_latitude', 'pickup_longitude']]\n",
    "with ProgressBar():\n",
    "    result = pickup.compute()  # compute to smaller Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(title=\"Pickup Locations\")\n",
    "p.scatter(result.pickup_longitude, result.pickup_latitude, size=3, alpha=0.2)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Store text/string data more efficiently with categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, a considerable portion of access time is consumed by accessing string data. This is because Pandas represents text with the **object dtype** which holds a normal Python string. \n",
    "\n",
    "Object dtypes are usually the main cause of slow code because **object dtypes run at Python speeds, not at Pandas’ normal C speeds**.\n",
    "\n",
    "Pandas **categoricals** are a new and powerful feature that **encodes categorical data into a numerical representation** so that we can leverage Pandas’ fast C code on this kind of text data.\n",
    "\n",
    "Given this, there are some strategies we can use in order to encode string data more efficiently which will **improve performance**.\n",
    "\n",
    "The strategy will demonstrate how to convert string data into categoricals. *This works very well when the data does not have a large number of distinct string values for a given column*.\n",
    "\n",
    "This takes a bit more time up front, but results in better performance.\n",
    "\n",
    "More on categoricals at the [pandas docs](http://pandas-docs.github.io/pandas-docs-travis/categorical.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ddf_hdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will categorise the 'vendor_id' and 'store_and_fwd_flag' columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize data\n",
    "ddf_hdf = ddf_hdf.categorize(columns=['vendor_id','store_and_fwd_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ddf_hdf.to_hdf(file_path + 'trip_data_1a_categorical.h5', '/data', format='table', mode='w', data_columns=['trip_distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now notice that about a 25% reduction in disk space has been realized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare again our categorised HDF5, with the previous execution times:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This may run slower on your computer depending on how full your RAM is from the previous datasets - try restarting the kernel and comparing the execution times from this point on without performing all the previous steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "#import castra as c\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 20, 10\n",
    "rcParams['font.size'] = 20\n",
    "\n",
    "rcParams['figure.dpi'] = 350\n",
    "rcParams['lines.linewidth'] = 2\n",
    "rcParams['axes.facecolor'] = 'white'\n",
    "rcParams['patch.edgecolor'] = 'white'\n",
    "rcParams['font.family'] = 'StixGeneral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'D:/158739_2019/2019_Python_notebooks/Block_2/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_csv = dd.read_csv(file_path + 'trip_data_1a.csv', parse_dates=['pickup_datetime', 'dropoff_datetime'])\n",
    "ddf_hdf = dd.read_hdf(file_path + 'trip_data_1a_categorical.h5', '/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_hdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with previous execution runtimes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ddf_hdf.passenger_count.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time len(ddf_hdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ddf_hdf[['vendor_id','passenger_count']].groupby('vendor_id').count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ddf_hdf.sample(frac=0.01)\n",
    "pickup = sample[['pickup_latitude', 'pickup_longitude']]\n",
    "\n",
    "with ProgressBar():\n",
    "    result = pickup.compute()  # compute to smaller Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fully appreciate and highlight the performance improvement of using categoricals, we will perform queries on categorical data that is in RAM.\n",
    "\n",
    "Let's consider the medallion column which has not been categorised yet. Let's load it into RAM and perform a query on the original:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ddf_hdf[['medallion','trip_distance']].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Perform a groupby operation on this new dataframe and time the operation that groups the dataframe by medallion and sums the trip_distances for each medallion, and sorts the results in a descending order, showing the top ten medallions (taxis) by total distance travelled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Convert the medallion columns into a category data type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Perform the same group by operation as above. Time the execution and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ddf_csv\n",
    "del ddf_hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Efficient On-Disk Storage Combined with Columnar and Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We saw the significant speed improvement realised by resorting to HDF5. The disadvantage with HDF5 is that all the column data is stored together in contiguous blocks, which means that it is inefficient when a dataset comprises of numerous columns and frequently, only subsets of columns must be accessed.  \n",
    "\n",
    "Therefore, there are more performance improvements that we can realise.\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "Good on-disk storage for DataFrames generally has the following characteristics\n",
    "\n",
    "1.  Binary storage format:  We don't store values as text that needs to be parsed as integers.  This rules out CSV and JSON.\n",
    "2.  Categorical support:  Text is hard in Python.  Pandas Categoricals can speed this up when the text is often repeated.\n",
    "3.  **Compression:  Light compression can often increase data bandwidth from disk.  Difficult to do right and depends largely on your data characteristics.**\n",
    "2.  **Column Stores:  We store individual columns separately and in chunks so that queries on just a few columns can load a small fraction of the dataset.  This rules out typical use of HDF5.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   bcolz Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> bcolz provides columnar, chunked data containers that can be compressed either in-memory and on-disk. Column storage allows for efficiently querying tables, as well as for cheap column addition and removal. It is based on NumPy, and uses it as the standard data container to communicate with bcolz objects, but it also comes with support for import/export facilities to/from HDF5/PyTables tables and pandas dataframes.\n",
    "\n",
    "> bcolz objects are compressed by default not only for reducing memory/disk storage, but also to improve I/O speed. The compression process is carried out internally by Blosc, a high-performance, multithreaded meta-compressor that is optimized for binary data (although it works with text data just fine too).\n",
    "\n",
    "> bcolz can also use numexpr internally (it does that by default if it detects numexpr installed) or dask so as to accelerate many vector and query operations (although it can use pure NumPy for doing so too). numexpr/dask can optimize the memory usage and use multithreading for doing the computations, so it is blazing fast. This, in combination with carray/ctable disk-based, compressed containers, can be used for performing out-of-core computations efficiently, but most importantly transparently.\n",
    "\n",
    "> By using compression, you can deal with more data using the same amount of memory, which is very good on itself. But in case you are wondering about the price to pay in terms of performance, you should know that nowadays memory access is the most common bottleneck in many computational scenarios, and that CPUs spend most of its time waiting for data. Hence, having data compressed in memory can reduce the stress of the memory subsystem as well.\n",
    "\n",
    "> Furthermore, columnar means that the tabular datasets are stored column-wise order, and this turns out to offer better opportunities to improve compression ratio. This is because data tends to expose more similarity in elements that sit in the same column rather than those in the same row, so compressors generally do a much better job when data is aligned in such column-wise order. In addition, when you have to deal with tables with a large number of columns and your operations only involve some of them, a columnar-wise storage tends to be much more effective because minimizes the amount of data that travels to CPU caches.\n",
    "\n",
    "> So, the ultimate goal for bcolz is not only reducing the memory needs of large arrays/tables, but also making bcolz operations to go faster than using a traditional data container like those in NumPy or Pandas. That is actually already the case in some real-life scenarios (see the notebook above) but that will become pretty more noticeable in combination with forthcoming, faster CPUs integrating more cores and wider vector units.\n",
    "\n",
    "> source: https://github.com/Blosc/bcolz\n",
    "\n",
    "Blosc is a compression library that is often used with bcolz:\n",
    "\n",
    "> Blosc (http://blosc.org) is a high performance compressor optimized for binary data. It has been designed to transmit data to the processor cache faster than the traditional, non-compressed, direct memory fetch approach via a memcpy() OS call.\n",
    "\n",
    "> Blosc works well for compressing numerical arrays that contains data with relatively low entropy, like sparse data, time series, grids with regular-spaced values, etc.\n",
    "\n",
    "> python-blosc a Python package that wraps Blosc. python-blosc supports Python 2.7 and 3.4 or higher versions.\n",
    "\n",
    "> source: https://github.com/Blosc/python-blosc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it makes sense to store large data in SQLite (or other fully fledged Database Management Systems like MySQL, Postgres, MongoDB etc. who are running as a separate process )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting our dataset to SQLite can be done by chunking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_hdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path + 'trip_data_1a.csv', infer_datetime_format=True, parse_dates=['pickup_datetime', 'dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('pickup_datetime')\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.to_hdf(file_path + 'trip_data_1a_pickup_datetime_index.h5', '/data', format='table', mode='w', data_columns=['trip_distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_datetime_index_hdf = dd.read_hdf(file_path + 'trip_data_1a_pickup_datetime_index.h5', '/data')\n",
    "ddf_datetime_index_hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_datetime_index_hdf.loc['2013-1-1'].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunksize = 10\n",
    "day = 1\n",
    "date = '2013-1-'\n",
    "time = ' 23:59:59'\n",
    "total = 0\n",
    "\n",
    "start = dt.datetime.now()\n",
    "conn = sqlite3.connect(file_path + \"trip_data_1a_a.sl3\")\n",
    "while (day <= 20):\n",
    "    temp = ddf_datetime_index_hdf.loc[date + str(day) : date + str(day) + time].compute()\n",
    "    temp.reset_index(inplace=True)\n",
    "    temp.to_sql('TAXITRIPDATA', conn, if_exists='append', index=False)\n",
    "    day += chunksize\n",
    "    total += len(temp)\n",
    "    print('{} seconds: completed {} rows inserted'.format((dt.datetime.now() - start).seconds, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['2013-1-1'])+len(df['2013-1-11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, alternatively if you have a csv that needs to be converted into a sql file, this can also be done by chunking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunksize = 100000\n",
    "total = 0\n",
    "start = dt.datetime.now()\n",
    "\n",
    "conn2 = sqlite3.connect(file_path + \"trip_data_1a_2.sl3\")\n",
    "for df in pd.read_csv(file_path + \"trip_data_1a.csv\", chunksize=chunksize):\n",
    "    df.to_sql('TAXITRIPDATA', conn2, if_exists='append', index=False)\n",
    "    total += len(df)\n",
    "    print('{} seconds: completed {} rows inserted'.format((dt.datetime.now() - start).seconds, total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are modules like **odo** that also automate the conversion process from multiple data types. See [`Odo documentation`](http://odo.pydata.org/en/latest/uri.html)\n",
    "\n",
    "Once a sqlite database file has been created, it can be opened and queried using the SQL language and out-of-core-processing within pandas as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(file_path + \"trip_data_1a_2.sl3\")\n",
    "curs = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sql = pd.read_sql_query('SELECT * FROM TAXITRIPDATA LIMIT 10', conn)\n",
    "df_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = dt.datetime.now()\n",
    "df_sql = pd.read_sql_query('SELECT medallion, pickup_datetime, dropoff_datetime, passenger_count '\n",
    "                           'FROM TAXITRIPDATA '\n",
    "                           'WHERE passenger_count > 5 '\n",
    "                           'LIMIT 30', conn)\n",
    "print('Operation took {} seconds.'.format((dt.datetime.now() - start).seconds))\n",
    "df_sql.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's time the operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = dt.datetime.now()\n",
    "df_sql = pd.read_sql_query('SELECT medallion, COUNT(*) as `num_journeys` '\n",
    "                           'FROM TAXITRIPDATA '\n",
    "                           'GROUP BY medallion '\n",
    "                           'ORDER BY -num_journeys '\n",
    "                           'LIMIT 30', conn)\n",
    "print('Operation took {} seconds.'.format((dt.datetime.now() - start).seconds))\n",
    "df_sql.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is far from ideal.\n",
    "\n",
    "It's possible to improve the access time by creating an index on a key column.\n",
    "\n",
    "**WARNING:** This might take a bit of time... \n",
    "\n",
    "**Exercise:** Write the SQL statement that will create an index on the 'medallion' field':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Re-execute the SQL query from the above example and compare the execution time with the newly set up index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a new index on another variable. Experiment with the above query to see if the new index has improved the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that the database connection is closed once finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Strategies For Dealing with Inconvenient Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dataset > RAM, we have a number of options in order to move forward and realise computational efficiency:\n",
    "\n",
    "1. Chunking the dataset.\n",
    "2. Using out-of-core processing libraries\n",
    "3. Convert datasets to more efficient encodings for disk I/O operations during out-of-core processing\n",
    "4. Filter binary datasets and bring into RAM only subsets of data\n",
    "5. Look to optimise file encodings with the particular options they each provide - ie string categorisation \n",
    "6. Combine the above strategies with sampling. Use out-of-core processing to downsample before pass off the smaller dataset to `pandas` to process quickly in-memory.\n",
    "7. Set an index on important columns\n",
    "8. Use SQLite, or interface with a full-blown database management instance that is running in its own separate process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storage choices strongly impact performance.  \n",
    "\n",
    "This becomes even more critical when the data is large.\n",
    "\n",
    "Strategies for handling data > RAM were covered.\n",
    "\n",
    "Key points:\n",
    "- store large datasets in a binary format\n",
    "- ensure that the binary format supports search/filtering on key columns\n",
    "- categorise string columns for more efficient queries\n",
    "- consider using Dask for out-of-core processing as well as other emerging libraries (Bcolz)\n",
    "- consider converting your data into a SQLite format, or if needed, loading it up into a database management system (MySQL, Oracle, SQL Server, MongoDB, Postgres etc.)\n",
    "- investigate how you can optimise your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require(['base/js/utils'],\n",
    "function(utils) {\n",
    "   utils.load_extensions('calico-spell-check', 'calico-document-tools', 'calico-cell-tools');\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
