{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/lib/python3.6/site-packages/dask/dataframe/utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from  datetime import datetime, timedelta\n",
    "import gc\n",
    "import numpy as np, pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity='all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n",
    "         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n",
    "        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\n",
    "PRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 28 \n",
    "max_lags = 57\n",
    "tr_last = 1913  # 历史的最后一天\n",
    "fday = datetime(2016,4, 25)  # 预测的第一天\n",
    "fday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_csv(\"sell_prices.csv\", dtype = PRICE_DTYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = pd.read_csv(\"calendar.csv\", dtype = CAL_DTYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_csv(\"sales_train_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dt(is_train = True, nrows = None, first_day = 1200):\n",
    "    prices = pd.read_csv(\"sell_prices.csv\", dtype = PRICE_DTYPES)\n",
    "    for col, col_dtype in PRICE_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            prices[col] = prices[col].cat.codes.astype(\"int16\")  # 类似于labelencoder，将store_id和item_id数字化表示\n",
    "            prices[col] -= prices[col].min()  \n",
    "            \n",
    "    cal = pd.read_csv(\"calendar.csv\", dtype = CAL_DTYPES)\n",
    "    cal[\"date\"] = pd.to_datetime(cal[\"date\"])  #  转化为datetime类型\n",
    "    for col, col_dtype in CAL_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            cal[col] = cal[col].cat.codes.astype(\"int16\")  # 将event_name_1/event_type_1/event_name_2/event_type_2weekday数字化表示\n",
    "            cal[col] -= cal[col].min()  # NaN会转换为-1，该代表将NaN归为0，其他从1开始\n",
    "    \n",
    "    start_day = max(1 if is_train  else tr_last-max_lags, first_day)  # 预测的第一天\n",
    "    \n",
    "    # 处理train_validation文件\n",
    "    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]  # 选取历史数据，从first_day到tr_last（包含这一天）d_1200,d_1201\n",
    "    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
    "    dtype = {numcol:\"float32\" for numcol in numcols}   # d_1200为float32\n",
    "    dtype.update({col: \"category\" for col in catcols if col != \"id\"})  # 除id外，都为category类型\n",
    "    dt = pd.read_csv(\"sales_train_validation.csv\", \n",
    "                     nrows = nrows, usecols = catcols + numcols, dtype = dtype)  # 取catcols和numcols的列(只取了需要的历史天数销量数据)\n",
    "    \n",
    "    for col in catcols:\n",
    "        if col != \"id\":\n",
    "            dt[col] = dt[col].cat.codes.astype(\"int16\")  # 数字化\n",
    "            dt[col] -= dt[col].min()\n",
    "    \n",
    "    if not is_train:  # 不是训练集\n",
    "        for day in range(tr_last+1, tr_last+ 28 +1):  # 遍历预测的每一天\n",
    "            dt[f\"d_{day}\"] = np.nan  # 先填充为NaN\n",
    "    \n",
    "    dt = pd.melt(dt,\n",
    "                  id_vars = catcols,\n",
    "                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n",
    "                  var_name = \"d\",\n",
    "                  value_name = \"sales\")\n",
    "    \n",
    "    dt = dt.merge(cal, on= \"d\", copy = False)\n",
    "    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_DAY = 1500\n",
    "df = create_dt(is_train=True, first_day= FIRST_DAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fea(dt):\n",
    "    lags = [7, 28]\n",
    "    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n",
    "    for lag, lag_col in zip(lags, lag_cols):  # [(7, lag_7), (28, lag_28)]\n",
    "        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)  # 新增两列，同一商品前7天的单价和前28天的价格\n",
    "\n",
    "    wins = [7, 28]\n",
    "    for win in wins :\n",
    "        for lag,lag_col in zip(lags, lag_cols):\n",
    "            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean()) \n",
    "\n",
    "            # log=7，wins=28表示一个商品当前的参考价为：从当前时间7天前（如1.8号的七天前是1.1号）开始一共向上取28天，求这些天的均值\n",
    "    \n",
    "    date_features = {\n",
    "        \n",
    "        \"wday\": \"weekday\",\n",
    "        \"week\": \"weekofyear\",\n",
    "        \"month\": \"month\",\n",
    "        \"quarter\": \"quarter\",\n",
    "        \"year\": \"year\",\n",
    "        \"mday\": \"day\",\n",
    "#         \"ime\": \"is_month_end\",\n",
    "#         \"ims\": \"is_month_start\",\n",
    "    }\n",
    "    \n",
    "#     dt.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n",
    "    \n",
    "    for date_feat_name, date_feat_func in date_features.items():\n",
    "        if date_feat_name in dt.columns:\n",
    "            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n",
    "        else:\n",
    "            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")\n",
    "            \n",
    "\n",
    "create_fea(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)\n",
    "cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n",
    "useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\n",
    "train_cols = df.columns[~df.columns.isin(useless_cols)]\n",
    "X_train = df[train_cols]\n",
    "y_train = df[\"sales\"]\n",
    "\n",
    "np.random.seed(777)\n",
    "\n",
    "fake_valid_inds = np.random.choice(X_train.index.values, 1_000_000, replace = False)  # 从index中，随机选择1000000个数\n",
    "print(fake_valid_inds)\n",
    "print(len(fake_valid_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)  # train是valid剩下的id号\n",
    "train_data = xgb.DMatrix(X_train.loc[train_inds] , label = y_train.loc[train_inds])\n",
    "fake_valid_data = xgb.DMatrix(X_train.loc[fake_valid_inds], label = y_train.loc[fake_valid_inds],)# This is a random sample, we're not gonna apply any time series train-test-split tricks here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'eta':0.1,\n",
    "          'gamma':0.1,\n",
    "          'max_depth':8, \n",
    "          'subsample':1,\n",
    "          'lambda': 0.1,\n",
    "          'alpha':0.1,\n",
    "          'objective':'count:poisson', # 分类结果or概率值\n",
    "          'eval_metric':'rmse', #多分类mlogloss，二分类为logloss\n",
    "          # 'num_class':3, # 标签种类数，二分类无需设置\n",
    "          'min_child_weight':1,\n",
    "          'colsample_bytree':1,\n",
    "          'colsample_bylevel':1,\n",
    "          # 'scale_pos_weight':0.8, # 平衡正负样本比例，默认1\n",
    "          # 'max_delta_step':0,\n",
    "          # 'max_leaves': 0\n",
    "          'seed':666,    \n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watch_list = [(train_data,'train'), (fake_valid_data,'valid')]\n",
    "m_xgb = xgb.train(params, train_data, num_boost_round=5000, evals=watch_list, early_stopping_rounds=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_xgb.save_model(\"xgb_model.lgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_xgb = xgb.Booster(model_file='xgb_model.lgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [1.035, 1.03, 1.025]\n",
    "weights = [1/len(alphas)]*len(alphas)  # 权重，当前权重值一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = 0.\n",
    "\n",
    "for icount, (alpha, weight) in enumerate(zip(alphas, weights)):  # [(1.035, 0.333), (1.03, 0.333), (1.025, 0.333)]\n",
    "    # te是所有的数据，tst是部分数据\n",
    "    te = create_dt(False)  # is_train = False  原始数据\n",
    "    cols = [f\"F{i}\" for i in range(1,29)]\n",
    "\n",
    "    for tdelta in range(0, 28):\n",
    "        day = fday + timedelta(days=tdelta)  # 预测的具体哪一天，比如第1914那天的日期\n",
    "        print(icount, day)\n",
    "        tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()  \n",
    "        create_fea(tst)\n",
    "        tst = xgb.DMatrix(tst.loc[tst.date == day , train_cols])  # 需要预测的数据\n",
    "        te.loc[te.date == day, \"sales\"] = alpha*m_xgb.predict(tst) # magic multiplier by kyakovlev\n",
    "\n",
    "\n",
    "\n",
    "    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()  # 取出预测的数据\n",
    "#     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h), \n",
    "#                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]  # [F1,F2,F3,...F28]\n",
    "    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()  # 转变成submission的形式（只是index只有每种商品的预测值，没有分位数和汇总的一些）\n",
    "    te_sub.fillna(0., inplace = True) \n",
    "    te_sub.sort_values(\"id\", inplace = True)\n",
    "    te_sub.reset_index(drop=True, inplace = True)\n",
    "    # te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n",
    "    if icount == 0 :\n",
    "        sub = te_sub\n",
    "        sub[cols] *= weight\n",
    "    else:\n",
    "        sub[cols] += te_sub[cols]*weight\n",
    "    print(icount, alpha, weight)\n",
    "\n",
    "\n",
    "sub2 = sub.copy()\n",
    "sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
    "sub.to_csv(\"submission_xgb_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm as tqdm\n",
    "\n",
    "from ipywidgets import widgets, interactive, interact\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sales = pd.read_csv('sales_train_validation.csv')\n",
    "calendar_df = pd.read_csv('calendar.csv')\n",
    "submission_file = pd.read_csv('sample_submission.csv')\n",
    "sell_prices = pd.read_csv('sell_prices.csv')\n",
    "xgb_sub = pd.read_csv(\"submission_xgb_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_col = ['id']\n",
    "xgb_col.extend([f'd_{day}' for day in range(1914, 1914+28)])\n",
    "xgb_sub.columns = xgb_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = ['Total']\n",
    "train_sales['Total'] = 'Total'\n",
    "train_sales['state_cat'] = train_sales.state_id + \"_\" + train_sales.cat_id\n",
    "train_sales['state_dept'] = train_sales.state_id + \"_\" + train_sales.dept_id\n",
    "train_sales['store_cat'] = train_sales.store_id + \"_\" + train_sales.cat_id\n",
    "train_sales['store_dept'] = train_sales.store_id + \"_\" + train_sales.dept_id\n",
    "train_sales['state_item'] = train_sales.state_id + \"_\" + train_sales.item_id\n",
    "train_sales['item_store'] = train_sales.item_id + \"_\" + train_sales.store_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_eval = ['validation', 'evaluation']\n",
    "\n",
    "# creating lists for different aggregation levels\n",
    "total = ['Total']\n",
    "states = ['CA', 'TX', 'WI']\n",
    "num_stores = [('CA',4), ('TX',3), ('WI',3)]\n",
    "stores = [x[0] + \"_\" + str(y + 1) for x in num_stores for y in range(x[1])]\n",
    "print(stores)  # 商店名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['FOODS', 'HOBBIES', 'HOUSEHOLD']\n",
    "num_depts = [('FOODS',3), ('HOBBIES',2), ('HOUSEHOLD',2)]\n",
    "depts = [x[0] + \"_\" + str(y + 1) for x in num_depts for y in range(x[1])]\n",
    "state_cats = [state + \"_\" + cat for state in states for cat in cats]\n",
    "state_depts = [state + \"_\" + dept for state in states for dept in depts]\n",
    "store_cats = [store + \"_\" + cat for store in stores for cat in cats]\n",
    "store_depts = [store + \"_\" + dept for store in stores for dept in depts]\n",
    "print(state_cats)  # 州名+大类\n",
    "print(\"=================================================================\")\n",
    "print(store_depts)  # 商店名（含州）+小类（含大类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prods = list(train_sales.item_id.unique())  # 商品列表\n",
    "prod_state = [prod + \"_\" + state for prod in prods for state in states]  # 商品（含大类和小类）+州名\n",
    "prod_store = [prod + \"_\" + store for prod in prods for store in stores]  # 商品（含大类和小类）+商店名（含州）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quants = ['0.005', '0.025', '0.165', '0.250', '0.500', '0.750', '0.835', '0.975', '0.995']\n",
    "days = range(1, 1913 + 29)\n",
    "time_series_columns = [f'd_{i}' for i in days]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sales(name_list, group):\n",
    "    '''\n",
    "    This function returns a dataframe (sales) on the aggregation level given by name list and group\n",
    "    '''\n",
    "    # rows_ve = [(name + \"_X_\" + str(q) + \"_\" + ve, str(q)) for name in name_list for q in quants for ve in val_eval]\n",
    "    sales = train_sales.groupby(group)[time_series_columns].sum() #would not be necessary for lowest level\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quantile_dict(name_list = stores, group = 'store_id' ,X = False):\n",
    "    '''\n",
    "    This function writes creates sales data on given aggregation level, and then writes predictions to the global dictionary my_dict\n",
    "    '''\n",
    "    sales = create_sales(name_list, group)\n",
    "    sales = sales.iloc[:, 1864:] #using the last few months data only\n",
    "    sales_quants = pd.DataFrame(index = sales.index)  # 空ataFrame，初始化为空\n",
    "    for q in quants:\n",
    "        sales_quants[q] = np.quantile(sales, float(q), axis = 1)  # 增加分位数的列，如CA_FOODS计算196天的0.005分位数\n",
    "    full_mean = pd.DataFrame(np.mean(sales, axis = 1))  #  新的DataFrame，只有一列，表示每个index最近196天的均值\n",
    "    daily_means = pd.DataFrame(index = sales.index)  # 新的DataFrame，初始化为空\n",
    "    for i in range(7):\n",
    "        daily_means[str(i)] = np.mean(sales.iloc[:, i::7], axis = 1)   # 新增7列，一周内每天的均值，如最近196天CA_FOOD,周一的均值\n",
    "        \n",
    "    daily_factors = daily_means / np.array(full_mean)\n",
    "    \n",
    "    daily_factors = pd.concat([daily_factors, daily_factors, daily_factors, daily_factors], axis = 1)\n",
    "    daily_factors_np = np.array(daily_factors)\n",
    "\n",
    "    factor_df = pd.DataFrame(daily_factors_np, columns = submission_file.columns[1:])\n",
    "    factor_df.index = daily_factors.index\n",
    "\n",
    "    for i,x in enumerate(tqdm(sales_quants.index)):\n",
    "        for q in quants:\n",
    "            v = sales_quants.loc[x, q] * np.array(factor_df.loc[x, :])\n",
    "            if X:\n",
    "                my_dict[x + \"_X_\" + q + \"_validation\"] = v\n",
    "                my_dict[x + \"_X_\" + q + \"_evaluation\"] = v\n",
    "            else:\n",
    "                my_dict[x + \"_\" + q + \"_validation\"] = v\n",
    "                my_dict[x + \"_\" + q + \"_evaluation\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {}\n",
    "#adding prediction to my_dict on all 12 aggregation levels\n",
    "create_quantile_dict(total, 'Total', X=True) #1\n",
    "create_quantile_dict(states, 'state_id', X=True) #2\n",
    "create_quantile_dict(stores, 'store_id', X=True) #3\n",
    "create_quantile_dict(cats, 'cat_id', X=True) #4\n",
    "create_quantile_dict(depts, 'dept_id', X=True) #5\n",
    "create_quantile_dict(state_cats, 'state_cat') #6\n",
    "create_quantile_dict(state_depts, 'state_dept') #7\n",
    "create_quantile_dict(store_cats, 'store_cat') #8\n",
    "create_quantile_dict(store_depts, 'store_dept') #9\n",
    "create_quantile_dict(prods, 'item_id', X=True) #10\n",
    "create_quantile_dict(prod_state, 'state_item') #11\n",
    "create_quantile_dict(prod_store, 'item_store') #12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(my_dict)\n",
    "pred_df = pred_df.transpose()\n",
    "pred_df_reset = pred_df.reset_index()\n",
    "final_pred = pd.merge(pd.DataFrame(submission_file.id), pred_df_reset, left_on = 'id', right_on = 'index')\n",
    "del final_pred['index']\n",
    "final_pred = final_pred.rename(columns={0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7', 7: 'F8', 8: 'F9',\n",
    "                                        9: 'F10', 10: 'F11', 11: 'F12', 12: 'F13', 13: 'F14', 14: 'F15', 15: 'F16',\n",
    "                                        16: 'F17', 17: 'F18', 18: 'F19', 19: 'F20', 20: 'F21', 21: 'F22', \n",
    "                                        22: 'F23', 23: 'F24', 24: 'F25', 25: 'F26', 26: 'F27', 27: 'F28'})\n",
    "final_pred = final_pred.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred.to_csv('submission_xgb_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
